{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import Config\n",
    "from model import Transformer\n",
    "from tokenizer import Tokenizer\n",
    "import torch as t\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_latest_model_folder(model_dir, file_start):\n",
    "    model_files = [file for file in os.listdir(model_dir) if file_start in file]\n",
    "    model_files.sort()\n",
    "    return os.path.join(model_dir, model_files[-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'saved_runs/mod_digit_add_2024-09-30_18-37-33'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_dir = 'saved_runs'\n",
    "file_start = 'mod_digit_add_'\n",
    "config = Config()\n",
    "latest_model_folder = get_latest_model_folder(model_dir, file_start)\n",
    "latest_model_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 200,  400,  600,  800, 1000, 1200, 1400, 1600, 1800],\n",
      "       dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "# total models will be num_models + 1\n",
    "num_models = 10\n",
    "num_epochs = 2000\n",
    "\n",
    "first_model = Transformer(config)\n",
    "first_model.load_state_dict(t.load(latest_model_folder + '/init.pth')['model'])\n",
    "first_model.eval()\n",
    "\n",
    "\n",
    "models = [Transformer(config) for _ in range(num_models-1)]\n",
    "intervals = t.linspace(0, num_epochs, num_models + 1).int()[1:-1]\n",
    "print(intervals)\n",
    "weights = [t.load(latest_model_folder + f'/{num}.pth') for num in intervals] \n",
    "for i in range(len(weights)):\n",
    "    models[i].load_state_dict(weights[i]['model'])\n",
    "    models[i].eval()\n",
    "\n",
    "last_model = Transformer(config)\n",
    "last_model.load_state_dict(t.load(latest_model_folder + f'/final.pth')['model'])\n",
    "last_model.eval()\n",
    "\n",
    "tokenizer = Tokenizer(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(model, maxnum = 100, p = 113):\n",
    "\n",
    "    correct = 0\n",
    "    count = 0\n",
    "    for i in range(maxnum):\n",
    "        for j in range(maxnum):\n",
    "            count += 1\n",
    "            print(f\"Accuracy: {(correct/count):.2%}, Count: {count}\", end = '\\r')\n",
    "            correct_answer = (i + j) % p\n",
    "            lsi = [int(k) for k in str(i)]\n",
    "            lsj = [int(k) for k in str(j)]\n",
    "            question = lsi + [10] + lsj + [11]\n",
    "\n",
    "            ll = len(question)\n",
    "            #print(\"q:\", question)\n",
    "            pred = model.generate_greedy(question)\n",
    "            answer = pred[ll: -1]\n",
    "            try:\n",
    "                answer = int(tokenizer.detokenize(answer))\n",
    "            except:\n",
    "                continue\n",
    "            #print(\"p\", pred)\n",
    "            #print(\"a\", answer)\n",
    "\n",
    "            if answer == correct_answer:\n",
    "                correct += 1\n",
    "            \n",
    "            \n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating accuracy for the initial model\n",
      "Accuracy: 0.00%, Count: 10000\n",
      "\n",
      "Calculating accuracy for model 200\n",
      "Accuracy: 87.86%, Count: 10000\n",
      "\n",
      "Calculating accuracy for model 400\n",
      "Accuracy: 87.99%, Count: 10000\n",
      "\n",
      "Calculating accuracy for model 600\n",
      "Accuracy: 90.32%, Count: 10000\n",
      "\n",
      "Calculating accuracy for model 800\n",
      "Accuracy: 88.13%, Count: 10000\n",
      "\n",
      "Calculating accuracy for model 1000\n",
      "Accuracy: 89.24%, Count: 10000\n",
      "\n",
      "Calculating accuracy for model 1200\n",
      "Accuracy: 89.44%, Count: 10000\n",
      "\n",
      "Calculating accuracy for model 1400\n",
      "Accuracy: 90.88%, Count: 10000\n",
      "\n",
      "Calculating accuracy for model 1600\n",
      "Accuracy: 88.66%, Count: 10000\n",
      "\n",
      "Calculating accuracy for model 1800\n",
      "Accuracy: 91.55%, Count: 10000\n",
      "\n",
      "Calculating accuracy for the final model\n",
      "Accuracy: 89.75%, Count: 10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Calculating accuracy for the initial model\")\n",
    "get_accuracy(first_model)\n",
    "\n",
    "for i, model in enumerate(models):\n",
    "    print(f\"Calculating accuracy for model {intervals[i]}\")\n",
    "    get_accuracy(model)\n",
    "\n",
    "print(\"Calculating accuracy for the final model\")\n",
    "get_accuracy(last_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: [1, 10, 2, 1, 11, 2, 2, 12]\n"
     ]
    }
   ],
   "source": [
    "test_sentence = \"1+1=\"\n",
    "test_sentence_tokenized = tokenizer.tokenize(test_sentence)\n",
    "\n",
    "pred = model.generate_greedy([1, 10, 2,1, 11])\n",
    "print(\"Prediction:\", pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 10, 1, 11]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sentence_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformer-addition-length-generalization",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
