{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "fixed_digit: False\n",
      "Training for 3000 epochs\n",
      "Batch size: 256\n",
      "Learning rate: 0.001\n",
      "Train on 0.5 of the data\n",
      "Saving model every 5 epochs\n"
     ]
    }
   ],
   "source": [
    "from model import Transformer\n",
    "from config import Config\n",
    "from tokenizer import Tokenizer\n",
    "import torch as t\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_folder_at_index(model_dir, file_start, folder_index):\n",
    "    model_files = [file for file in os.listdir(model_dir) if file_start in file]\n",
    "    model_files.sort()\n",
    "    return os.path.join(model_dir, model_files[folder_index])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'saved_runs/mod_digit_add_2024-10-03_17-00-11'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_dir = 'saved_runs'\n",
    "file_start = 'mod_digit_add_'\n",
    "config = Config()\n",
    "folder_index = -1\n",
    "latest_model_folder = get_model_folder_at_index(model_dir, file_start, folder_index) \n",
    "latest_model_folder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 300,  600,  900, 1200, 1500, 1800, 2100, 2400, 2700],\n",
      "       dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "# total models will be num_models + 1\n",
    "num_models = 10\n",
    "num_epochs = 3000\n",
    "\n",
    "first_model = Transformer(config)\n",
    "first_model.load_state_dict(t.load(latest_model_folder + '/init.pth')['model'])\n",
    "first_model.eval()\n",
    "\n",
    "\n",
    "models = [Transformer(config) for _ in range(num_models-1)]\n",
    "intervals = t.linspace(0, num_epochs, num_models + 1).int()[1:-1]\n",
    "print(intervals)\n",
    "weights = [t.load(latest_model_folder + f'/models/{num}.pth') for num in intervals] \n",
    "for i in range(len(weights)):\n",
    "    models[i].load_state_dict(weights[i]['model'])\n",
    "    models[i].eval()\n",
    "\n",
    "last_model = Transformer(config)\n",
    "last_model.load_state_dict(t.load(latest_model_folder + f'/final.pth')['model'])\n",
    "last_model.eval()\n",
    "\n",
    "tokenizer = Tokenizer(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(model, maxnum = 100):\n",
    "\n",
    "    correct = 0\n",
    "    count = 0\n",
    "    for i in range(maxnum):\n",
    "        for j in range(maxnum):\n",
    "            count += 1\n",
    "            print(f\"Accuracy: {(correct/count):.2%}, Count: {count}\", end = '\\r')\n",
    "            correct_answer = (i + j)\n",
    "            lsi = [int(k) for k in str(i)]\n",
    "            lsj = [int(k) for k in str(j)]\n",
    "            question = lsi + [10] + lsj + [11]\n",
    "\n",
    "            ll = len(question)\n",
    "            #print(\"q:\", question)\n",
    "            pred = model.generate_greedy(question)\n",
    "            answer = pred[ll: -1]\n",
    "            try:\n",
    "                answer = int(tokenizer.detokenize(answer))\n",
    "            except:\n",
    "                continue\n",
    "            #print(\"p\", pred)\n",
    "            #print(\"a\", answer)\n",
    "\n",
    "            if answer == correct_answer:\n",
    "                correct += 1\n",
    "            \n",
    "            \n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating accuracy for the initial model\n",
      "Accuracy: 0.00%, Count: 10000\n",
      "\n",
      "Calculating accuracy for model 300\n",
      "Accuracy: 98.89%, Count: 10000\n",
      "\n",
      "Calculating accuracy for model 600\n",
      "Accuracy: 98.60%, Count: 10000\n",
      "\n",
      "Calculating accuracy for model 900\n",
      "Accuracy: 99.17%, Count: 10000\n",
      "\n",
      "Calculating accuracy for model 1200\n",
      "Accuracy: 99.14%, Count: 10000\n",
      "\n",
      "Calculating accuracy for model 1500\n",
      "Accuracy: 99.46%, Count: 10000\n",
      "\n",
      "Calculating accuracy for model 1800\n",
      "Accuracy: 99.45%, Count: 10000\n",
      "\n",
      "Calculating accuracy for model 2100\n",
      "Accuracy: 99.21%, Count: 10000\n",
      "\n",
      "Calculating accuracy for model 2400\n",
      "Accuracy: 99.62%, Count: 10000\n",
      "\n",
      "Calculating accuracy for model 2700\n",
      "Accuracy: 99.66%, Count: 10000\n",
      "\n",
      "Calculating accuracy for the final model\n",
      "Accuracy: 99.62%, Count: 10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Calculating accuracy for the initial model\")\n",
    "get_accuracy(first_model)\n",
    "\n",
    "for i, model in enumerate(models):\n",
    "    print(f\"Calculating accuracy for model {intervals[i]}\")\n",
    "    get_accuracy(model)\n",
    "\n",
    "print(\"Calculating accuracy for the final model\")\n",
    "get_accuracy(last_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/ubuntu/mert/aylin/transformer-addition-length-generalization/Inference.ipynb Cell 7\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B150.136.146.76/home/ubuntu/mert/aylin/transformer-addition-length-generalization/Inference.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m test_sentence \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m1+81=\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B150.136.146.76/home/ubuntu/mert/aylin/transformer-addition-length-generalization/Inference.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m test_sentence_tokenized \u001b[39m=\u001b[39m tokenizer\u001b[39m.\u001b[39mtokenize(test_sentence)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B150.136.146.76/home/ubuntu/mert/aylin/transformer-addition-length-generalization/Inference.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m pred \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mgenerate_greedy([\u001b[39m1\u001b[39m, \u001b[39m10\u001b[39m, \u001b[39m8\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m11\u001b[39m])\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B150.136.146.76/home/ubuntu/mert/aylin/transformer-addition-length-generalization/Inference.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mPrediction:\u001b[39m\u001b[39m\"\u001b[39m, pred)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "test_sentence = \"1+81=\"\n",
    "test_sentence_tokenized = tokenizer.tokenize(test_sentence)\n",
    "\n",
    "pred = model.generate_greedy([1, 10, 8, 1, 11])\n",
    "print(\"Prediction:\", pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 10, 1, 11]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sentence_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformer-addition-length-generalization",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
