{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "fixed_digit: True\n",
      "Training for 3000 epochs\n",
      "Batch size: 256\n",
      "Learning rate: 0.001\n",
      "Train on 0.2 of the data\n",
      "Saving model every 5 epochs\n"
     ]
    }
   ],
   "source": [
    "from model import Transformer\n",
    "from config import Config\n",
    "from tokenizer import Tokenizer\n",
    "import torch as t\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_folder_at_index(model_dir, file_start, folder_index):\n",
    "    model_files = [file for file in os.listdir(model_dir) if file_start in file]\n",
    "    model_files.sort()\n",
    "    return os.path.join(model_dir, model_files[folder_index])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'saved_runs/mod_digit_add_2024-10-01_09-20-00'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_dir = 'saved_runs'\n",
    "file_start = 'mod_digit_add_'\n",
    "config = Config()\n",
    "folder_index = -3\n",
    "latest_model_folder = get_model_folder_at_index(model_dir, file_start, folder_index) + '/models/'\n",
    "latest_model_folder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 200,  400,  600,  800, 1000, 1200, 1400, 1600, 1800],\n",
      "       dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "# total models will be num_models + 1\n",
    "num_models = 10\n",
    "num_epochs = 2000\n",
    "\n",
    "first_model = Transformer(config)\n",
    "first_model.load_state_dict(t.load(latest_model_folder + '/init.pth')['model'])\n",
    "first_model.eval()\n",
    "\n",
    "\n",
    "models = [Transformer(config) for _ in range(num_models-1)]\n",
    "intervals = t.linspace(0, num_epochs, num_models + 1).int()[1:-1]\n",
    "print(intervals)\n",
    "weights = [t.load(latest_model_folder + f'/{num}.pth') for num in intervals] \n",
    "for i in range(len(weights)):\n",
    "    models[i].load_state_dict(weights[i]['model'])\n",
    "    models[i].eval()\n",
    "\n",
    "last_model = Transformer(config)\n",
    "last_model.load_state_dict(t.load(latest_model_folder + f'/final.pth')['model'])\n",
    "last_model.eval()\n",
    "\n",
    "tokenizer = Tokenizer(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(model, maxnum = 100, p = 113):\n",
    "\n",
    "    correct = 0\n",
    "    count = 0\n",
    "    for i in range(maxnum):\n",
    "        for j in range(maxnum):\n",
    "            count += 1\n",
    "            print(f\"Accuracy: {(correct/count):.2%}, Count: {count}\", end = '\\r')\n",
    "            correct_answer = (i + j) % p\n",
    "            lsi = [int(k) for k in str(i)]\n",
    "            lsj = [int(k) for k in str(j)]\n",
    "            question = lsi + [10] + lsj + [11]\n",
    "\n",
    "            ll = len(question)\n",
    "            #print(\"q:\", question)\n",
    "            pred = model.generate_greedy(question)\n",
    "            answer = pred[ll: -1]\n",
    "            try:\n",
    "                answer = int(tokenizer.detokenize(answer))\n",
    "            except:\n",
    "                continue\n",
    "            #print(\"p\", pred)\n",
    "            #print(\"a\", answer)\n",
    "\n",
    "            if answer == correct_answer:\n",
    "                correct += 1\n",
    "            \n",
    "            \n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating accuracy for the initial model\n",
      "Accuracy: 0.00%, Count: 10000\n",
      "\n",
      "Calculating accuracy for model 200\n",
      "Accuracy: 92.80%, Count: 10000\n",
      "\n",
      "Calculating accuracy for model 400\n",
      "Accuracy: 95.99%, Count: 10000\n",
      "\n",
      "Calculating accuracy for model 600\n",
      "Accuracy: 96.88%, Count: 10000\n",
      "\n",
      "Calculating accuracy for model 800\n",
      "Accuracy: 96.23%, Count: 10000\n",
      "\n",
      "Calculating accuracy for model 1000\n",
      "Accuracy: 97.60%, Count: 10000\n",
      "\n",
      "Calculating accuracy for model 1200\n",
      "Accuracy: 98.11%, Count: 10000\n",
      "\n",
      "Calculating accuracy for model 1400\n",
      "Accuracy: 97.57%, Count: 10000\n",
      "\n",
      "Calculating accuracy for model 1600\n",
      "Accuracy: 99.03%, Count: 10000\n",
      "\n",
      "Calculating accuracy for model 1800\n",
      "Accuracy: 99.07%, Count: 10000\n",
      "\n",
      "Calculating accuracy for the final model\n",
      "Accuracy: 99.12%, Count: 10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Calculating accuracy for the initial model\")\n",
    "get_accuracy(first_model)\n",
    "\n",
    "for i, model in enumerate(models):\n",
    "    print(f\"Calculating accuracy for model {intervals[i]}\")\n",
    "    get_accuracy(model)\n",
    "\n",
    "print(\"Calculating accuracy for the final model\")\n",
    "get_accuracy(last_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: [1, 10, 1, 11, 2, 12]\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "test_sentence = \"1+1=\"\n",
    "test_sentence_tokenized = tokenizer.tokenize(test_sentence)\n",
    "\n",
    "pred = model.generate_greedy([1, 10,1, 11])\n",
    "print(\"Prediction:\", pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 10, 1, 11]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sentence_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformer-addition-length-generalization",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
